{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xray_view_position_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcg6dZiPy9sM"
      },
      "source": [
        "# **X-ray view position classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt4dY7nb1n8V"
      },
      "source": [
        "To exemplify the most basic **AI pipeline** (the \"hello world\" pipeline for AI), let's try to create an [X-ray view position classifier](https://github.com/amandafbri/xray-view-position-classifier)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw-bqQbMorUd"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAR3svkNzZpZ"
      },
      "source": [
        "##**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1hmCddVzNgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0fef16-919c-4ed9-898a-e1d9be54af37"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlPEOJ_Rz988"
      },
      "source": [
        "!pip install autokeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD6OAOalzfXZ"
      },
      "source": [
        "import numpy as np\n",
        "import autokeras as ak\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVEp7G08ydfa"
      },
      "source": [
        "#**DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpdz7uR919j7"
      },
      "source": [
        "##**Load and prepare data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gtc8KIX4vPK"
      },
      "source": [
        "Dataset:\n",
        "\n",
        "https://github.com/ieee8023/covid-chestxray-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us_n1kkJ4fc6"
      },
      "source": [
        "How did I splitted the data and organized the directories:\n",
        "\n",
        "https://github.com/amandafbri/xray-view-position-classifier/blob/main/mount_csv.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEfQrulipK1p"
      },
      "source": [
        "AutoKeras [`image_dataset_from_directory`](https://autokeras.com/utils/) is equivalent to [`ImageDataGenerator`](https://keras.io/api/preprocessing/image/#imagedatagenerator-class) from Keras, so let's use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtFXSdss2C4Z"
      },
      "source": [
        "train_directory = '/content/drive/MyDrive/Estudo/Colab Notebooks/data/xray_view_position/Train'\n",
        "validation_directory = '/content/drive/MyDrive/Estudo/Colab Notebooks/data/xray_view_position/Validation'\n",
        "test_directory = '/content/drive/MyDrive/Estudo/Colab Notebooks/data/xray_view_position/Test'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8WvMgLa8kz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d1849d-aa8f-482e-bb8a-cbc84c660ac8"
      },
      "source": [
        "# Configurable parameters\n",
        "train_batch_size = 128\n",
        "validation_test_batch_size = 32\n",
        "img_height = 512\n",
        "img_width = 512\n",
        "\n",
        "# Set seed to ensure the same split when loading testing data.\n",
        "seed = 123\n",
        "\n",
        "train_data = ak.image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    seed=seed,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=train_batch_size,\n",
        ")\n",
        "\n",
        "validation_data = ak.image_dataset_from_directory(\n",
        "    validation_directory,\n",
        "    seed=seed,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=validation_test_batch_size,\n",
        ")\n",
        "\n",
        "test_data = ak.image_dataset_from_directory(\n",
        "    test_directory,\n",
        "    seed=seed,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=validation_test_batch_size,\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 692 files belonging to 4 classes.\n",
            "Found 86 files belonging to 4 classes.\n",
            "Found 87 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KusNUVIkqUNv",
        "outputId": "cf721a3c-07ed-4eae-8b2c-97663ade2777"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 512, 512, 3), (None,)), types: (tf.float32, tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfPFYNo3yjNe"
      },
      "source": [
        "#**AUTOML**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY-Nb-Dg8NAN"
      },
      "source": [
        "##**Create and train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpxehCqvu0Hr"
      },
      "source": [
        "AutoKeras automatically detects a lot of the [classifier parameters](https://autokeras.com/image_classifier/), but it is always recommended to pass them explicitly for more transparency and understanding of what is going on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffDbsxJs8UqV"
      },
      "source": [
        "clf = ak.ImageClassifier(\n",
        "    max_trials=1,\n",
        "    directory='/content/drive/MyDrive/Estudo/Colab Notebooks/xray_view_classifier_v2',\n",
        "    objective='val_loss',\n",
        "    seed=seed,\n",
        "    num_classes=4,\n",
        "    multi_label=False,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1nnNoPgvrkD"
      },
      "source": [
        "The `fit` method search for the best model and hyperparameters. It also has a default `Early Stopping` method (10 epochs of patience)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4Ru5rEd8pU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d34605-52ee-4ace-8804-5df30bb88b91"
      },
      "source": [
        "clf.fit(\n",
        "    x=train_data,\n",
        "    epochs=10,\n",
        "    validation_data=validation_data,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [00h 16m 07s]\n",
            "val_loss: 0.996360182762146\n",
            "\n",
            "Best val_loss So Far: 0.996360182762146\n",
            "Total elapsed time: 00h 16m 07s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/10\n",
            "Not enough memory, reduce batch size to 64.\n",
            "Epoch 1/10\n",
            "Not enough memory, reduce batch size to 32.\n",
            "Epoch 1/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 18.3137 - accuracy: 0.4523 - auc: 0.6774 - val_loss: 1.0195 - val_accuracy: 0.4651 - val_auc: 0.8044\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 0.7102 - accuracy: 0.7197 - auc: 0.9148 - val_loss: 0.7956 - val_accuracy: 0.6047 - val_auc: 0.8784\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 0.3045 - accuracy: 0.9118 - auc: 0.9880 - val_loss: 0.8701 - val_accuracy: 0.6279 - val_auc: 0.8765\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 0.1133 - accuracy: 0.9783 - auc: 0.9979 - val_loss: 1.0394 - val_accuracy: 0.5930 - val_auc: 0.8609\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 0.0593 - accuracy: 0.9884 - auc: 0.9993 - val_loss: 1.2353 - val_accuracy: 0.6279 - val_auc: 0.8679\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 0.0595 - accuracy: 0.9884 - auc: 0.9980 - val_loss: 1.2145 - val_accuracy: 0.6512 - val_auc: 0.8702\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 0.0648 - accuracy: 0.9913 - auc: 0.9975 - val_loss: 0.9559 - val_accuracy: 0.7209 - val_auc: 0.9059\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 83s 3s/step - loss: 0.0530 - accuracy: 0.9928 - auc: 0.9971 - val_loss: 1.0201 - val_accuracy: 0.6512 - val_auc: 0.8813\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 84s 3s/step - loss: 0.0327 - accuracy: 0.9913 - auc: 0.9998 - val_loss: 1.2754 - val_accuracy: 0.6047 - val_auc: 0.8594\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 84s 3s/step - loss: 0.0351 - accuracy: 0.9913 - auc: 0.9987 - val_loss: 0.9920 - val_accuracy: 0.7558 - val_auc: 0.8957\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Estudo/Colab Notebooks/xray_view_classifier_v2/image_classifier/best_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEsflMu5hpJY"
      },
      "source": [
        "It took 16 minutes for the first trial with best `val_loss` equals to 0.99"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifzJnsSmIuQG"
      },
      "source": [
        "best_model = clf.tuner.get_best_model()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Gfhfwkf4vW"
      },
      "source": [
        "best_model.save('/content/drive/MyDrive/Estudo/Colab Notebooks/xray_view_classifier_v2/bestmodel_v2.h5')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMnbCcm3wkCB"
      },
      "source": [
        "##**Loading the trained model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsRyY9TMxB3E"
      },
      "source": [
        "Let's take a look into our best model trained by AutoKeras!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs1Wfr6cwvnc"
      },
      "source": [
        "trained_model = tf.keras.models.load_model('/content/drive/MyDrive/Estudo/Colab Notebooks/xray_view_classifier_v2/bestmodel_v2.tf')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcoPM1Izw0nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe61fde5-6205-42ed-a0b2-18510ff1e064"
      },
      "source": [
        "trained_model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 512, 512, 3)]     0         \n",
            "_________________________________________________________________\n",
            "cast_to_float32 (CastToFloat (None, 512, 512, 3)       0         \n",
            "_________________________________________________________________\n",
            "normalization (Normalization (None, 512, 512, 3)       7         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 510, 510, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 508, 508, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 254, 254, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 254, 254, 64)      0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4129024)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4129024)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 16516100  \n",
            "_________________________________________________________________\n",
            "classification_head_1 (Softm (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 16,535,499\n",
            "Trainable params: 16,535,492\n",
            "Non-trainable params: 7\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS8do3iUkHYB",
        "outputId": "b6a765e9-dff4-493a-984d-6c477d37ffd1"
      },
      "source": [
        "trained_model.get_config()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_layers': [['input_1', 0, 0]],\n",
              " 'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 512, 512, 3),\n",
              "    'dtype': 'float32',\n",
              "    'name': 'input_1',\n",
              "    'ragged': False,\n",
              "    'sparse': False},\n",
              "   'inbound_nodes': [],\n",
              "   'name': 'input_1'},\n",
              "  {'class_name': 'Custom>CastToFloat32',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'cast_to_float32',\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['input_1', 0, 0, {}]]],\n",
              "   'name': 'cast_to_float32'},\n",
              "  {'class_name': 'Normalization',\n",
              "   'config': {'axis': (-1,),\n",
              "    'dtype': 'float32',\n",
              "    'name': 'normalization',\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['cast_to_float32', 0, 0, {}]]],\n",
              "   'name': 'normalization'},\n",
              "  {'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 32,\n",
              "    'groups': 1,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (3, 3),\n",
              "    'name': 'conv2d',\n",
              "    'padding': 'valid',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True},\n",
              "   'inbound_nodes': [[['normalization', 0, 0, {}]]],\n",
              "   'name': 'conv2d'},\n",
              "  {'class_name': 'Conv2D',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'data_format': 'channels_last',\n",
              "    'dilation_rate': (1, 1),\n",
              "    'dtype': 'float32',\n",
              "    'filters': 64,\n",
              "    'groups': 1,\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'kernel_size': (3, 3),\n",
              "    'name': 'conv2d_1',\n",
              "    'padding': 'valid',\n",
              "    'strides': (1, 1),\n",
              "    'trainable': True,\n",
              "    'use_bias': True},\n",
              "   'inbound_nodes': [[['conv2d', 0, 0, {}]]],\n",
              "   'name': 'conv2d_1'},\n",
              "  {'class_name': 'MaxPooling2D',\n",
              "   'config': {'data_format': 'channels_last',\n",
              "    'dtype': 'float32',\n",
              "    'name': 'max_pooling2d',\n",
              "    'padding': 'valid',\n",
              "    'pool_size': (2, 2),\n",
              "    'strides': (2, 2),\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['conv2d_1', 0, 0, {}]]],\n",
              "   'name': 'max_pooling2d'},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.25,\n",
              "    'seed': None,\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['max_pooling2d', 0, 0, {}]]],\n",
              "   'name': 'dropout'},\n",
              "  {'class_name': 'Flatten',\n",
              "   'config': {'data_format': 'channels_last',\n",
              "    'dtype': 'float32',\n",
              "    'name': 'flatten',\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['dropout', 0, 0, {}]]],\n",
              "   'name': 'flatten'},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout_1',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.5,\n",
              "    'seed': None,\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['flatten', 0, 0, {}]]],\n",
              "   'name': 'dropout_1'},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'linear',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense',\n",
              "    'trainable': True,\n",
              "    'units': 4,\n",
              "    'use_bias': True},\n",
              "   'inbound_nodes': [[['dropout_1', 0, 0, {}]]],\n",
              "   'name': 'dense'},\n",
              "  {'class_name': 'Softmax',\n",
              "   'config': {'axis': -1,\n",
              "    'dtype': 'float32',\n",
              "    'name': 'classification_head_1',\n",
              "    'trainable': True},\n",
              "   'inbound_nodes': [[['dense', 0, 0, {}]]],\n",
              "   'name': 'classification_head_1'}],\n",
              " 'name': 'model',\n",
              " 'output_layers': [['classification_head_1', 0, 0]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ghj49kCnBnO",
        "outputId": "ebb0211b-55e9-4348-e166-c1bca35736b7"
      },
      "source": [
        "trained_model.get_layer('cast_to_float32')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<autokeras.keras_layers.CastToFloat32 at 0x7fbd2afddc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHbbIBVzw5kW"
      },
      "source": [
        "##**Predicting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5isT4mxmEB"
      },
      "source": [
        "img = image.load_img('/content/drive/MyDrive/Estudo/Colab Notebooks/data/xray_view_position/Test/L/covid-19-caso-91-2-13.png', target_size=(512,512))\n",
        "img_in_array = image.img_to_array(img)\n",
        "img_in_array_expanded = np.expand_dims(img_in_array, axis=0)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZxK9FC0odnU"
      },
      "source": [
        "It's important to note that the expanded dimension is necessary to pass through the model, because it represents the batch size parameter (even though you'll pass just one image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHwPjfHooYWe",
        "outputId": "65abc8f1-2527-49f0-de0a-60ba1cc67e4d"
      },
      "source": [
        "img_in_array_expanded.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 512, 512, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM6aDA_2xYLq"
      },
      "source": [
        "classes = trained_model.predict(img_in_array_expanded, batch_size=validation_test_batch_size)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMxnSHPro6E3"
      },
      "source": [
        "It predicted correctly as a X-ray with lateral view position!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W_gUuN4x6zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efed603a-1f55-4534-e9e4-3110a2b1d1c2"
      },
      "source": [
        "classes.round()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kVEr56dymS7"
      },
      "source": [
        "#**DEPLOY**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8NMxDbJy58r"
      },
      "source": [
        "There are a lot of possibilities here and some free resources, but first we need to create our simple API."
      ]
    }
  ]
}